import { GoogleGenerativeAI } from '@google/generative-ai'
import { NextRequest } from 'next/server'

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '')

// System prompt chi ti·∫øt cho tr·ª£ l√Ω h·ªçc ti·∫øng Nh·∫≠t
const SYSTEM_PROMPT = `B·∫°n l√† AnAn - tr·ª£ l√Ω AI th√¥ng minh c·ªßa website AnAn Nihongo, chuy√™n h·ªó tr·ª£ h·ªçc ti·∫øng Nh·∫≠t.

## Vai tr√≤ c·ªßa b·∫°n:
- Gi·∫£i ƒë√°p th·∫Øc m·∫Øc v·ªÅ ti·∫øng Nh·∫≠t (ng·ªØ ph√°p, t·ª´ v·ª±ng, Kanji, Hiragana, Katakana)
- H·ªó tr·ª£ ng∆∞·ªùi d√πng hi·ªÉu b√†i h·ªçc ƒëang xem (context b√†i h·ªçc s·∫Ω ƒë∆∞·ª£c cung c·∫•p trong [])
- ƒê∆∞a ra v√≠ d·ª• th·ª±c t·∫ø v√† d·ªÖ hi·ªÉu
- Khuy·∫øn kh√≠ch v√† t·∫°o ƒë·ªông l·ª±c h·ªçc t·∫≠p

## Quy t·∫Øc tr·∫£ l·ªùi:
1. **Ng√¥n ng·ªØ**: Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, d·ªÖ hi·ªÉu
2. **ƒê·ªãnh d·∫°ng**: S·ª≠ d·ª•ng Markdown (bold, italic, bullet points, b·∫£ng)
3. **Kanji**: Lu√¥n k√®m phi√™n √¢m Hiragana trong ngo·∫∑c v√† nghƒ©a ti·∫øng Vi·ªát
   V√≠ d·ª•: ÂãâÂº∑ („Åπ„Çì„Åç„Çá„ÅÜ - h·ªçc t·∫≠p)
4. **Ng·∫Øn g·ªçn**: Tr·∫£ l·ªùi s√∫c t√≠ch, ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ
5. **V√≠ d·ª•**: ƒê∆∞a v√≠ d·ª• c√¢u ti·∫øng Nh·∫≠t khi gi·∫£i th√≠ch ng·ªØ ph√°p

## Khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ b√†i h·ªçc:
- Tham kh·∫£o context trong [] ƒë·ªÉ bi·∫øt h·ªç ƒëang h·ªçc g√¨
- Gi·∫£i th√≠ch li√™n quan ƒë·∫øn n·ªôi dung b√†i h·ªçc ƒë√≥
- C√≥ th·ªÉ g·ª£i √Ω link b√†i h·ªçc n·∫øu ƒë∆∞·ª£c cung c·∫•p

## Phong c√°ch:
- Th√¢n thi·ªán, nhi·ªát t√¨nh nh∆∞ m·ªôt gia s∆∞
- S·ª≠ d·ª•ng emoji ph√π h·ª£p üìö‚ú®üéå
- Khen ng·ª£i khi ng∆∞·ªùi d√πng h·ªèi hay ho·∫∑c ti·∫øn b·ªô`

export async function POST(req: NextRequest) {
    try {
        const { messages, screenContext } = await req.json()

        if (!process.env.GEMINI_API_KEY) {
            return new Response(
                JSON.stringify({ error: 'GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh' }),
                { status: 500, headers: { 'Content-Type': 'application/json' } }
            )
        }

        const model = genAI.getGenerativeModel({ 
            model: 'gemini-2.5-flash',
            systemInstruction: SYSTEM_PROMPT,
        })

        // Gi·ªõi h·∫°n history ch·ªâ l·∫•y 4 tin nh·∫Øn g·∫ßn nh·∫•t (2 c·∫∑p h·ªèi-ƒë√°p)
        const recentMessages = messages.slice(-5) // 4 tin c≈© + 1 tin m·ªõi
        let history = recentMessages.slice(0, -1).map((msg: any) => ({
            role: msg.role === 'user' ? 'user' : 'model',
            parts: [{ text: msg.content.slice(0, 500) }], // Gi·ªõi h·∫°n m·ªói tin 500 k√Ω t·ª±
        }))

        // ƒê·∫£m b·∫£o history b·∫Øt ƒë·∫ßu v·ªõi role 'user', n·∫øu kh√¥ng th√¨ b·ªè qua ph·∫ßn ƒë·∫ßu
        if (history.length > 0 && history[0].role !== 'user') {
            history = history.slice(1)
        }

        // Get the latest user message
        const latestMessage = messages[messages.length - 1].content

        // Add screen context if available (gi·ªõi h·∫°n 150 k√Ω t·ª±)
        let contextualPrompt = latestMessage.slice(0, 500) // Gi·ªõi h·∫°n c√¢u h·ªèi 500 k√Ω t·ª±
        if (screenContext && screenContext.length > 0) {
            const shortContext = screenContext.slice(0, 350)
            contextualPrompt = `[${shortContext}]\n${contextualPrompt}`
        }

        const chat = model.startChat({
            history: history.length > 0 ? history : undefined,
            generationConfig: {
                temperature: 0.7,
                maxOutputTokens: 1024,
            },
        })

        // Use streaming
        const result = await chat.sendMessageStream(contextualPrompt)

        // Create a readable stream for the response
        const encoder = new TextEncoder()
        const stream = new ReadableStream({
            async start(controller) {
                try {
                    for await (const chunk of result.stream) {
                        const text = chunk.text()
                        if (text) {
                            controller.enqueue(encoder.encode(`data: ${JSON.stringify({ text })}\n\n`))
                        }
                    }
                    controller.enqueue(encoder.encode('data: [DONE]\n\n'))
                    controller.close()
                } catch (error) {
                    controller.error(error)
                }
            },
        })

        return new Response(stream, {
            headers: {
                'Content-Type': 'text/event-stream',
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
            },
        })
    } catch (error: any) {
        console.error('Chat API Error:', error)
        
        // Handle rate limit error specifically
        if (error.status === 429 || error.message?.includes('429') || error.message?.includes('quota')) {
            return new Response(
                JSON.stringify({ 
                    error: 'H·ªá th·ªëng ƒëang qu√° t·∫£i. Vui l√≤ng th·ª≠ l·∫°i sau 1 ph√∫t.',
                    code: 'RATE_LIMIT'
                }),
                { status: 429, headers: { 'Content-Type': 'application/json' } }
            )
        }
        
        return new Response(
            JSON.stringify({ error: error.message || 'C√≥ l·ªói x·∫£y ra' }),
            { status: 500, headers: { 'Content-Type': 'application/json' } }
        )
    }
}
